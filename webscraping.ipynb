{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "40035cd9-7749-4906-9315-13e79c497901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import eng_to_ipa as ipa\n",
    "import re\n",
    "import os \n",
    "\n",
    "pd.set_option('display.max.rows', None)\n",
    "pd.set_option('display.max.columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a página: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_vocabulary_urls():\n",
    "    url = \"https://www.languageguide.org/inglês/vocabulário/\"\n",
    "    \n",
    "    soup = get_html(url)\n",
    "\n",
    "    # Lista para armazenar os dados\n",
    "    data = []\n",
    "    \n",
    "    # Itera sobre todas as divs com a classe 'section'\n",
    "    for section in soup.find_all('div', class_='section'):\n",
    "        # Pega o título da seção\n",
    "        title_section = section.find('span', class_='title').get_text(strip=True)\n",
    "        \n",
    "        # Itera sobre os elementos com a classe 'category-link'\n",
    "        category_links = section.find_all('div', class_='category-link')\n",
    "        for link in category_links:\n",
    "            # Pega o link e o texto dentro da categoria\n",
    "            a_tag = link.find('a')\n",
    "            if a_tag:\n",
    "                href = f\"https://www.languageguide.org{a_tag.get('href')}\"\n",
    "                text = a_tag.get_text(strip=True)\n",
    "                data.append({\n",
    "                    'type': 'category-link',\n",
    "                    'section': title_section,\n",
    "                    'subsection': text,\n",
    "                    'url': href\n",
    "                })\n",
    "        \n",
    "        # Verifica se existe algum link com a classe 'notepad2'\n",
    "        notepad_links = section.find_all('a', class_='notepad2')\n",
    "        if notepad_links:\n",
    "            for notepad in notepad_links:\n",
    "                notepad_href = f\"https://www.languageguide.org{notepad.get('href')}\"\n",
    "                notepad_text = notepad.get_text(strip=True)\n",
    "                data.append({\n",
    "                    'type': 'notepad2',\n",
    "                    'section': title_section,\n",
    "                    'subsection': notepad_text,\n",
    "                    'url': notepad_href\n",
    "                })\n",
    "        else:\n",
    "            # Se não tiver 'notepad2', continue com os próximos 'category-link'\n",
    "            for link in category_links:\n",
    "                a_tag = link.find('a')\n",
    "                if a_tag:\n",
    "                    href = f\"https://www.languageguide.org{a_tag.get('href')}\"\n",
    "                    text = a_tag.get_text(strip=True)\n",
    "                    data.append({\n",
    "                        'type': 'category-link',\n",
    "                        'section': title_section,\n",
    "                        'subsection': text,\n",
    "                        'url': href\n",
    "                    })\n",
    "    \n",
    "    # Cria um DataFrame\n",
    "    df = pd.DataFrame(data, columns=['type', 'section', 'subsection', 'url'])\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def transform_urls(df):\n",
    "    df.loc[13, 'subsection'] = 'The Skeleton(O esqueleto)'\n",
    "    df.loc[138, 'subsection'] = 'Continents(Os continentes)'\n",
    "    df.loc[139, 'subsection'] = 'Europe(Europa)'\n",
    "    df.loc[:, 'subsection'] = df['subsection'].str.replace('(', ' (')\n",
    "    df.loc[:, 'section'] = df['section'].str.replace('(', ' (')\n",
    "    return df\n",
    "\n",
    "\n",
    "def _split_concatenated_words(text):\n",
    "    # Regex para separar entre minúsculas e maiúsculas\n",
    "    split_text = re.findall(r'[A-Z][a-z]*', text)\n",
    "    return split_text\n",
    "\n",
    "\n",
    "def transform_words(df):\n",
    "    # correção pontual\n",
    "    df.loc[df['português'] == 'O olho <br> Os olhos', 'português'] = 'O olho'\n",
    "\n",
    "    # count words\n",
    "    df['count'] = df['inglês'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "    #separando palavras juntas\n",
    "    condition = df['subseção']=='Europe (Europa)'\n",
    "    df.loc[condition, 'inglês'] = df.loc[condition, 'inglês'].apply(_split_concatenated_words)\n",
    "    df = df.explode('inglês').reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def scrap_words(soup, subsection, section):\n",
    "    data = []\n",
    "\n",
    "    # Primeiro padrão: busca palavras e traduções dentro das tags <div>\n",
    "    words = soup.find_all(\"div\", class_=\"pop_up\")\n",
    "    translations = soup.find_all(\"div\", class_=\"trans_popup\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # Limpeza dos dados (remove espaços em branco e descarta valores vazios)\n",
    "    words = [word.text.strip() for word in words if word.text.strip() != '']\n",
    "    translations = [translation.text.strip() for translation in translations if translation.text.strip() != '']\n",
    "\n",
    "    if not translations:\n",
    "        translations = ['' for i in range(len(words))]\n",
    "    \n",
    "    # Adiciona os pares de palavra-tradução ao data (primeiro padrão)\n",
    "    for word, translation in zip(words, translations):\n",
    "        data.append({\n",
    "            \"seção\": section,  \n",
    "            \"subseção\": subsection,     \n",
    "            \"português\": translation,\n",
    "            \"inglês\": word,\n",
    "            \"ipa\": ''  # Mantém o campo IPA vazio, se necessário preencher depois\n",
    "        })\n",
    "    \n",
    "    # Segundo padrão: busca palavras e traduções dentro das tags <tr> e <td>\n",
    "    rows = soup.find_all(\"tr\", class_=\"audio\")\n",
    "    \n",
    "    # Itera sobre cada linha (tr) para pegar os pares de palavra-tradução\n",
    "    for row in rows:\n",
    "        tds = row.find_all(\"td\")\n",
    "\n",
    "        # Verifica se há duas células (td), uma para a palavra e outra para a tradução\n",
    "        if len(tds) >= 2:\n",
    "            word = tds[0].find(\"span\", class_=\"w\").text.strip() if tds[0].find(\"span\", class_=\"w\") else None\n",
    "            translation = tds[1].find(\"span\", class_=\"translation\").text.strip() if tds[1].find(\"span\", class_=\"translation\") else None\n",
    "            \n",
    "            # Se ambos forem encontrados, adiciona os dados à lista (segundo padrão)\n",
    "            if word and translation:\n",
    "                data.append({\n",
    "                    \"seção\": section,  \n",
    "                    \"subseção\": subsection,     \n",
    "                    \"português\": translation,\n",
    "                    \"inglês\": word,\n",
    "                    \"ipa\": ''  # Deixe o campo IPA vazio\n",
    "                })\n",
    "    \n",
    "    # Terceiro padrão: busca palavras e traduções dentro de tags <li> com classes específicas\n",
    "    list_items = soup.find_all(\"tr\", class_=\"audio\")\n",
    "    \n",
    "    # Itera sobre os itens de lista (li) para pegar os pares de palavra-tradução\n",
    "    for item in list_items:\n",
    "        word = item.find(\"span\", class_=\"word\").text.strip() if item.find(\"span\", class_=\"word\") else None\n",
    "        translation = item.find(\"span\", class_=\"translation\").text.strip() if item.find(\"span\", class_=\"translation\") else None\n",
    "        \n",
    "        # Se ambos forem encontrados, adiciona os dados à lista (terceiro padrão)\n",
    "        if word and translation:\n",
    "            data.append({\n",
    "                \"seção\": section,\n",
    "                \"subseção\": subsection,\n",
    "                \"português\": translation,\n",
    "                \"inglês\": word,\n",
    "                \"ipa\": ''  # Deixe o campo IPA vazio\n",
    "            })\n",
    "\n",
    "    blockquotes = soup.find_all(\"blockquote\")\n",
    "\n",
    "    for blockquote in blockquotes:\n",
    "        audio_divs = blockquote.find_all(\"div\", class_=\"audio\")\n",
    "        \n",
    "        for audio_div in audio_divs:\n",
    "            word = audio_div.find(\"span\", class_=\"w\").text.strip() if audio_div.find(\"span\", class_=\"w\") else None\n",
    "            translation = audio_div.find(\"span\", class_=\"translation\").text.strip() if audio_div.find(\"span\", class_=\"translation\") else None\n",
    "            \n",
    "            # Se ambos forem encontrados, adiciona os dados à lista (terceiro padrão)\n",
    "            if word and translation:\n",
    "                data.append({\n",
    "                    \"seção\": section,  \n",
    "                    \"subseção\": subsection,     \n",
    "                    \"português\": translation,\n",
    "                    \"inglês\": word,\n",
    "                    \"ipa\": ''  # Deixe o campo IPA vazio\n",
    "                })\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_words(df):\n",
    "\n",
    "    data = []\n",
    "    for  _, row in df.iterrows():\n",
    "        \n",
    "        type = row['type']\n",
    "        subsection = row['subsection']\n",
    "        section = row['section']\n",
    "        url = row['url']\n",
    "\n",
    "        print(section, subsection)\n",
    "    \n",
    "        soup = get_html(url)\n",
    "        register = scrap_words(soup, subsection, section)\n",
    "        data = data + register\n",
    "    return pd.DataFrame(data).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "def transcribe_to_ipa(word):\n",
    "    return ipa.convert(word)\n",
    "\n",
    "def ipa_transform(df):\n",
    "    \n",
    "    condition = df['count'] <= 2  \n",
    "    df.loc[condition, 'ipa']= df.loc[condition, 'inglês'].apply(transcribe_to_ipa)\n",
    "    df['ipa_limpa'] = df['ipa'].str.replace(r\"[ˈˈˌ*]\", \"\", regex=True).replace(r'[^\\w\\s]', '', regex=True)\n",
    "    \n",
    "    df['ipa_limpa_reversa'] = df['ipa_limpa'].apply(lambda x: str(x)[::-1])\n",
    "    df = df.sort_values(['count', 'ipa_limpa_reversa']).reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "95ffd7a1-b674-4386-9f2c-69115e799672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_files = os.listdir('data')\n",
    "\n",
    "########################################## GET VOCABULARY URLS  ########################################## \n",
    "if 'urls.csv' in data_files:\n",
    "    raw = pd.read_csv('data/urls.csv')\n",
    "\n",
    "else:\n",
    "    # get urls\n",
    "    df_urls = get_vocabulary_urls()\n",
    "    df_urls = transform_urls(df_urls)\n",
    "    df_urls.to_csv('data/urls.csv', index=False)\n",
    "\n",
    "\n",
    "########################################## GET WORDS  ########################################## \n",
    "# get words in each url\n",
    "if 'raw.csv' in data_files:\n",
    "    raw = pd.read_csv('data/raw.csv')\n",
    "\n",
    "else:\n",
    "    words_df = get_words(df_urls)\n",
    "    df = transform_words(words_df)\n",
    "    df.to_csv('data/raw.csv', index=False)\n",
    "\n",
    "########################################## GET IPA   ########################################## \n",
    "if 'ipa.xlsx' in data_files:\n",
    "    ipa_df = pd.read_excel('data/ipa.xlsx')\n",
    "    \n",
    "else:\n",
    "    # getting IPA and save\n",
    "    ipa_df = ipa_transform(df)\n",
    "    ipa_df.to_excel('data/ipa.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "17bdc54d-8b96-44d3-b4e2-17499ef3bf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4018"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipa_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "5c6aa3bd-9113-4e49-9ad1-ef340774c7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify missing subsections\n",
    "df_urls[~df_urls['subsection'].isin(words_df['subseção'])]['url'].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
